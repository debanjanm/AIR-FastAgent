{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6196896a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.pydantic_v1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BM25Retriever \u001b[38;5;66;03m# Import is still needed for loading\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define the file path where the retriever is saved\u001b[39;00m\n\u001b[1;32m      5\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbm25_retriever.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my-experiments/lib/python3.13/site-packages/langchain_community/retrievers/__init__.py:187\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _module_lookup:\n\u001b[0;32m--> 187\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_module_lookup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my-experiments/lib/python3.13/importlib/__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my-experiments/lib/python3.13/site-packages/langchain_community/retrievers/bm25.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CallbackManagerForRetrieverRun\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Field\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseRetriever\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_preprocessing_func\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.pydantic_v1'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from langchain_community.retrievers import BM25Retriever # Import is still needed for loading\n",
    "\n",
    "# Define the file path where the retriever is saved\n",
    "file_path = \"bm25_retriever.pkl\"\n",
    "\n",
    "# Load the BM25Retriever using pickle\n",
    "with open(file_path, \"rb\") as f:\n",
    "    loaded_bm25_retriever = pickle.load(f)\n",
    "\n",
    "print(f\"BM25Retriever loaded from {file_path}\")\n",
    "\n",
    "# You can now use the loaded_bm25_retriever\n",
    "# query = \"lazy dog\"\n",
    "query = \"Explain PopularityAdjusted Block Model (PABM)\"\n",
    "relevant_docs = loaded_bm25_retriever.invoke(query)\n",
    "for doc in relevant_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up environment variables for LM Studio\n",
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:1234/v1/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd79df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a LangChain pipeline with prompt template and LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# choose model name exactly as LM Studio exposes it (check LM Studio UI)\n",
    "# llm = ChatOpenAI(model=\"qwen/qwen3-4b-thinking-2507\", temperature=0.2)  \n",
    "\n",
    "llm = ChatOpenAI(model=\"qwen/qwen3-4b-2507\", temperature=0)  \n",
    "\n",
    "\n",
    "template = \"Answer the Question based on the context below.\\n\\nContext: {context}\\n\\nQ: {question}\\nA:\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# prompt = PromptTemplate(input_variables=[\"q\"], template=\"Context: Q: {q}\\nA:\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# print(chain.run(\"Explain recursion in 2 sentences.\"))\n",
    "\n",
    "# print(chain.invoke({\"context\": relevant_docs, \"question\": \"Explain PopularityAdjusted Block Model (PABM)\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------------------------##\n",
    "\n",
    "# pip install -qU \"langchain[anthropic]\" to call the model\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# def get_weather(city: str) -> str:\n",
    "#     \"\"\"Get weather for a given city.\"\"\"\n",
    "#     return f\"It's always sunny in {city}!\"\n",
    "\n",
    "def retrieve_context(query: str) -> str:\n",
    "    \"\"\"Retrieve context for a given query using the loaded BM25Retriever.\"\"\"\n",
    "    docs = loaded_bm25_retriever.invoke(query)\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[retrieve_context],\n",
    "    system_prompt=\"You are a helpful assistant. you have access to a tool that retrieves relevant context based on user queries.\",\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "sl = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"who are you?\"}]}\n",
    ")\n",
    "\n",
    "print(\"Agent Response:{}\".format(sl))\n",
    "import json\n",
    "print(type(sl))\n",
    "\n",
    "\n",
    "# pretty_json_string = json.dumps(sl, indent=4)\n",
    "# print(pretty_json_string)\n",
    "\n",
    "##------------------------------------------------------------------------------##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl.get('messages')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver \n",
    "\n",
    "# def get_weather(city: str) -> str:\n",
    "#     \"\"\"Get weather for a given city.\"\"\"\n",
    "#     return f\"It's always sunny in {city}!\"\n",
    "\n",
    "def retrieve_context(query: str) -> str:\n",
    "    \"\"\"Retrieve context for a given query using the loaded BM25Retriever.\"\"\"\n",
    "\n",
    "    file_path = \"bm25_retriever.pkl\"\n",
    "\n",
    "    # Load the BM25Retriever using pickle\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        loaded_bm25_retriever = pickle.load(f)\n",
    "    docs = loaded_bm25_retriever.invoke(query)\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[retrieve_context],\n",
    "    system_prompt=\"You are a helpful assistant. you have access to a tool that retrieves relevant context based on user queries.\",\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "sl = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Summarize Our Conversation\"}]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "final_response = agent.invoke({\"messages\": \"hi, my name is Debanjan\"}, config)\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "final_response = agent.invoke({\"messages\": \"Explain PopularityAdjusted Block Model (PABM)\"}, config)\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "final_response = agent.invoke({\"messages\": \"Explain Model Structure and Assumptions\"}, config)\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# print(\"Agent Response:{}\".format(sl))\n",
    "\n",
    "# response = sl.get('messages')[-1]\n",
    "\n",
    "# print(\"------------------------------\")\n",
    "# print(\"Agent Response:{}\".format(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade23001",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d669eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.agents.middleware import before_model\n",
    "from langgraph.runtime import Runtime\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "@before_model\n",
    "def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Keep only the last few messages to fit context window.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if len(messages) <= 3:\n",
    "        return None  # No changes needed\n",
    "\n",
    "    first_msg = messages[0]\n",
    "    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\n",
    "    new_messages = [first_msg] + recent_messages\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
    "            *new_messages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[retrieve_context],\n",
    "    middleware=[trim_messages],\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "\"\"\"\n",
    "================================== Ai Message ==================================\n",
    "\n",
    "Your name is Bob. You told me that earlier.\n",
    "If you'd like me to call you a nickname or use a different name, just say the word.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43d13f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
